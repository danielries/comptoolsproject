{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup for PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "client_id = '0X3RFkechF94pb0jhrBaBA'\n",
    "client_secret = '_A0RRGRXo4w_rWKm6mGYeoqqvW2NnA'\n",
    "user_agent='MyRedditBot:v1.0 (by u/Healthy-Pollution929)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the PRAW client\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "\n",
    "def fetch_comments(subreddit_name, search_query, time_filter='year', post_limit=200, comment_limit=10, sleep_time=1):\n",
    "    \"\"\"\n",
    "    Fetch the top 10 most upvoted comments from posts on a specific topic in a given subreddit.\n",
    "    \n",
    "    Parameters:\n",
    "        subreddit_name (str): Name of the subreddit to search.\n",
    "        search_query (str): The search term to filter posts.\n",
    "        time_filter (str): Time range for the search ('day', 'week', 'month', 'year', 'all').\n",
    "        post_limit (int): Maximum number of posts to retrieve.\n",
    "        comment_limit (int): Maximum number of comments to retrieve per post.\n",
    "        sleep_time (int): Time in seconds to wait between each request batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing collected comment data.\n",
    "    \"\"\"\n",
    "    comments_data = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    print(f\"Collecting comments from posts in r/{subreddit_name} related to '{search_query}'...\")\n",
    "\n",
    "    try:\n",
    "        # Search for relevant posts\n",
    "        for post in subreddit.search(search_query, time_filter=time_filter, limit=post_limit):\n",
    "            # Set comment sort order to \"top\" to get the highest upvoted comments\n",
    "            post.comment_sort = 'top'\n",
    "            \n",
    "            # Get the post permalink to construct URLs\n",
    "            post_url = f\"https://www.reddit.com{post.permalink}\"\n",
    "\n",
    "            # Get the top comments for each post\n",
    "            post.comments.replace_more(limit=0)\n",
    "            for comment in post.comments[:comment_limit]:  # Limit to top 10 comments per post\n",
    "                if comment.body.strip():  # Only include non-empty comments\n",
    "                    # Construct the direct URL to the comment\n",
    "                    comment_url = f\"{post_url}{comment.id}\"\n",
    "\n",
    "                    # Store the comment data in a dictionary\n",
    "                    comment_info = {\n",
    "                        \"created_date\": pd.to_datetime(comment.created_utc, unit='s'),\n",
    "                        \"subreddit_id\": subreddit_name,\n",
    "                        \"search_query\": search_query,\n",
    "                        \"post_id\": post.id,\n",
    "                        \"comment_id\": comment.id,\n",
    "                        \"post_title\": post.title,\n",
    "                        \"comment_text\": comment.body,\n",
    "                        \"upvotes\": comment.ups,\n",
    "                        \"post_url\": post_url,         \n",
    "                        \"comment_url\": comment_url  \n",
    "                    }\n",
    "\n",
    "                    # Append the comment data to the list\n",
    "                    comments_data.append(comment_info)\n",
    "\n",
    "            # Sleep to prevent hitting Reddit's rate limit\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    # Handle API exceptions\n",
    "    except praw.exceptions.RedditAPIException as api_error:\n",
    "        print(f\"Rate limit or other API error: {api_error}\")\n",
    "        time.sleep(60) \n",
    "\n",
    "    # Handle other exceptions\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error occurred: {e}\")\n",
    "        time.sleep(10) \n",
    "\n",
    "    # Convert collected data to DataFrame\n",
    "    df_comments = pd.DataFrame(comments_data)\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\"created_date\", \"subreddit_id\", \"search_query\", \"post_id\", \"comment_id\", \n",
    "                    \"post_title\", \"comment_text\", \"upvotes\", \"post_url\", \"comment_url\"]\n",
    "    df_comments = df_comments[column_order]\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "# Define parameters\n",
    "subreddit_name = 'Israel'  # Specify one subreddit\n",
    "search_query = 'Palestine'  # Single topic\n",
    "\n",
    "# Fetch comments for one subreddit and topic\n",
    "df_comments = fetch_comments(subreddit_name=subreddit_name, search_query=search_query)\n",
    "\n",
    "# Optional: Save the data to a CSV file\n",
    "df_comments.to_csv(f'data/{subreddit_name}_{search_query}_comments.csv', index=False)\n",
    "\n",
    "print(f\"Collected {len(df_comments_israel)} comments on '{search_query}' from r/{subreddit_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "df_politics_Israel = pd.read_csv('data/politics_Israel_comments.csv')\n",
    "df_politics_Palestine = pd.read_csv('data/politics_Palestine_comments.csv')\n",
    "df_worldnews_Israel = pd.read_csv('data/worldnews_Israel_comments.csv')\n",
    "df_worldnews_Palestine = pd.read_csv('data/worldnews_Palestine_comments.csv')\n",
    "df_Israel_Israel = pd.read_csv('data/Israel_Israel_comments.csv')\n",
    "df_Israel_Palestine = pd.read_csv('data/Israel_Palestine_comments.csv')\n",
    "df_Palestine_Israel = pd.read_csv('data/Palestine_Israel_comments.csv')\n",
    "df_Palestine_Palestine = pd.read_csv('data/Palestine_Palestine_comments.csv')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df_combined = pd.concat([\n",
    "    df_politics_Israel, df_politics_Palestine, \n",
    "    df_worldnews_Israel, df_worldnews_Palestine, \n",
    "    df_Israel_Israel, df_Israel_Palestine, \n",
    "    df_Palestine_Israel, df_Palestine_Palestine\n",
    "])\n",
    "\n",
    "# Print initial shape\n",
    "print(f\"Combined DataFrame shape before processing: {df_combined.shape}\")\n",
    "\n",
    "# Store original text in a new column for reference\n",
    "df_combined['comment_original'] = df_combined['comment_text']\n",
    "\n",
    "# Remove duplicate comments based on 'comment_text'\n",
    "df_combined = df_combined.drop_duplicates(subset='comment_text')\n",
    "\n",
    "# Remove comments with 1 or fewer upvotes\n",
    "df_combined = df_combined[df_combined['upvotes'] > 1]\n",
    "\n",
    "# Text preprocessing steps applied to 'comment_text'\n",
    "# Remove URLs\n",
    "df_combined['comment_text'] = df_combined['comment_text'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))\n",
    "\n",
    "# Lowercase conversion\n",
    "df_combined['comment_text'] = df_combined['comment_text'].str.lower()\n",
    "\n",
    "# Remove special characters\n",
    "df_combined['comment_text'] = df_combined['comment_text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "# Remove stop words (make sure nltk stopwords are downloaded)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_combined['comment_text'] = df_combined['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Renaming\n",
    "df_combined.rename(columns={'comment_text': 'comment_processed'}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "df_combined = df_combined[['created_date', 'subreddit_id', 'search_query', 'post_id', 'comment_id', \n",
    "                           'post_title', 'comment_original', 'comment_processed', 'upvotes', \n",
    "                           'post_url', 'comment_url']]\n",
    "\n",
    "# Reset index\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print shape after processing\n",
    "print(f\"Combined DataFrame shape after processing: {df_combined.shape}\")\n",
    "\n",
    "# Display final DataFrame\n",
    "df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example to compare original comment with processed comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a comment URL and text\n",
    "print(df_combined['comment_url'].iloc[0])\n",
    "print(df_combined['comment_original'].iloc[0])\n",
    "print(df_combined['comment_processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from afinn import Afinn\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load AFINN lexicon with defaultdict to handle missing words\n",
    "sent_lexicon = defaultdict(lambda: 0, Afinn()._dict)\n",
    "\n",
    "# Function to calculate sentiment score for a single text\n",
    "def calculate_sentiment(text):\n",
    "\n",
    "    print(*(sent_lexicon[word] for word in text), sep='\\n')\n",
    "    \n",
    "    # Sum up the sentiment scores of each word in the text\n",
    "    sentiment_score = sum(sent_lexicon[word] for word in text)\n",
    "    \n",
    "    # Optionally normalize by the number of words (average sentiment per word)\n",
    "    if len(text) > 0:\n",
    "        normalized_score = sentiment_score / len(text)\n",
    "    else:\n",
    "        normalized_score = 0\n",
    "    \n",
    "    return normalized_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputationalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
